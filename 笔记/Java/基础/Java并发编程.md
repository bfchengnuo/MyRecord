# Java并发编程

内容大部分来自《Java并发编程的艺术》，其他少量来自网络进行补充。

## 前置知识

并发编程中，上下文的切换是主要的耗时操作，因为涉及内核态的转换，减少上下文切换的几种方案：

- 无锁并发编程：例如根据 hash 分段
- CAS
- 最少线程
- 协程：单线程实现多任务调度，单线程处理多个任务的切换，参考 Go 的调度

优化建议：使用 jstack 命令 dump 线程查看 Waiting 的线程，减少其数量。

避免死锁：

- 避免一个线程获得多个锁
- 尽量保证一个锁只占用一个资源
- 尝试使用定时锁，`lock.tryLock(timeout)`
- 对于数据库锁，加锁解锁在一个连接中，否则可能会解锁失败

资源限制问题：

并不是多线程速度就一定快，IO、带宽这些是固定的，再多线程也不能超出，并且还多了上下文切换的开销，很容易 CPU 爆掉；可以使用大数据的集群方案缓解压力，要么就注意控制并发的数量，复用资源等。

### 对象头结构

其中分析的最多的就是 MarkWord，在 32 位和 64 位的 JVM 中是不一样的：

| 长度      | 内容                   | 说明                       |
| --------- | ---------------------- | -------------------------- |
| 32/64 bit | Mark Word              | 存储对象 hash 或者锁信息等 |
| 32/64 bit | Class Metadata Address | 存储到对象类型数据的指针   |
| 32/32 bit | Array Length           | 数组的长度，如果是数组的话 |

下面是 MarkWord 的结构，分为 32 bit 和 64 bit：

32 位因为长度不够会进行一定的复用，毕竟 JVM 规范没有规定具体的结构：

![](../../../img/java/markword-32.jpg)

如果是 64 bit，就比较规整：

![](../../../img/java/markword-64.png)

### 处理器如何实现原子操作

主要通过两种方式：

1. 锁缓存（优先）：锁定一块内存区域，其他 CPU 不能对其进行访问；
2. 锁总线：把 CPU     对内存的通信锁定，锁定期间其他 CPU 不能对内存进行操作；

当 CPU 不支持锁缓存或者数据不能被缓存到 CPU，亦或者操作的数据需要跨多个缓存行，就只能锁总线；

Java 使用锁或者 CAS 保证原子操作；不过使用 CAS 会有几个限制：

- ABA 问题，JDK 提供带版本的     AtomicStampedReference 来解决；
- 循环时间如果过长会开销很大
- 只能保证一个变量的原子操作，JDK5 后提供     AtomicReference 保证引用对象的原子性，可以把多个变量放到对象中；

## 再谈Volatile

这个已经被面试搞的来基本都知道一点，基本的东西就不说了，不过注意区别操作系统术语『缓存行填充』和『缓冲行』的区别；

Volatile 最终会编译为 lock 指令（非字节码层面），多核 CPU 架构对这个指令的规定是：

- 将当前处理器缓存行的数据写回到主存（lock 指令锁总线或者缓存）
- 这个写回主存的操作会让其他     CPU 缓存了这个地址的数据无效（通过总线事件）

其他 CPU 的感知是通过缓存一致性协议，每个处理器会嗅探总线的消息来检测自己缓存的数据是不是过期了；如果过期了就会设置为无效状态，用到的话就只能从主存获取；

并发大师 Doug Lea 在早期的 LinkedTransferQueue 使用过『缓存行对齐』，即现代很多处理器的 L1 和 L2 都是 64 字节缓存行，定义一些字段填充到 64 避免被加载到一个缓存行；

如果加载到了一个缓存行，修改的时候会将整个缓存行锁定，影响效率，不过这种对齐方式并不是都建议使用 Volatile 的时候用，一方面有的 CPU 的 L1 和 L2 不是 64 字节，另一个如果不会被频繁的写，其实没太大必要；在 JDK7 之后 JVM 的优化会去除无用字段，所以也需要使用其他追加字节的手段。

另外一个老生常谈的问题，volatile 不能保证原子性（复合操作，对于单独的写和读是可以保证原子性的），i++ 并不是原子操作（一次读一次写），所以使用 volatile 修饰过也是不安全的，这里注意 volatile 是 Java 层面的，涉及的是 JMM，与 CPU 的缓存不要混了；

## 再谈Synchronized

这一部分由于面试的轰炸已经成为常识了吧，什么锁升级之类的，在深入 JVM 中也讲了不少，这里做补充；

它的锁升级主要依赖对象头也就是 MarkWord 的标记，占用 1 个字符宽度，32 位就是 32 bit，64 就是 64 bit 了；

偏向锁：

![](../../../img/java/偏向锁.jpg)

轻量级锁：

![](../../../img/java/轻量级锁.jpg)

偏向锁是通过检测对象头来完成，基本可以看作是无锁的；主要解决的是**总是同一个线程获得锁（无竞争）情况**的优化；当发生竞争，会检测偏向的线程是否存活（偏向锁不会主动撤销），如果还在锁状态就会进行撤销偏向操作，这时候（32 位来说，具体看上面的对象头差异） Mark Word 中不再存放偏向线程 ID，而是存放 hashCode 和 GC 分代年龄，同时锁标识位变为“01”（表示未锁定）；

轻量级锁也是对象头配合，需要 copy 到线程栈，**通过自旋完成锁**，如果自旋时间过长或者出现第三个竞争者会膨胀为重量级；解决的是同步代码执行时间很短情况的优化；

| 锁       | 优点                                 | 缺点                                   | 场景                           |
| -------- | ------------------------------------ | -------------------------------------- | ------------------------------ |
| 偏向锁   | 基本没有消耗，跟非同步仅存纳秒级差距 | 如果存在竞争，会存在额外的撤销操作开销 | 只有一个线程访问同步代码块     |
| 轻量级锁 | 竞争的线程不会阻塞，提高了响应速度   | 始终得不到锁会有自旋的 CPU 开销        | 追求响应时间，同步块执行速度快 |
| 重量级锁 | 不使用自旋，不会消耗 CPU             | 阻塞，响应时间慢                       | 追求吞吐量，同步块执行耗时     |

![](../../../img/java/Synchronized原理.jpg)

## JMM

并发编程中，主要处理两个问题，线程之间**如何通信**和**如何同步**；

通信机制常用的两种是：共享内存和消息传递；同步就是指的线程之间操作顺序问题；

Java 并发采用了共享内存的模型，**线程之间的通信都是隐式进行**，对开发透明。

JMM 决定了线程对共享变量（例如堆中对象）的写入何时对其他线程可见；并且定义了主存和工作（本地）内存，本地内存存储了读写变量的**副本**，它是 JMM 抽象的一个概念，并不真实存在。

所以 JMM 下的线程通信是**通过主存来完成的**。

程序执行时，为了提高性能，会进行重排，重排分为三种：

1. 编译器优化的重排，编译器在不改变单线程语义的情况下，可以重安排语句执行顺序；
2. 指令级并行重排序，如果不存在数据依赖性，现代处理器都可以通过改变语句对应的机器指令的执行顺序。
3. 内存系统重排序，由于处理器使用了缓存技术来读写缓冲区，看起来加载和存储操作是乱序执行的。

对于 1 是编译器级别，2 和 3 是处理器重排序；这些重排序可能会导致可见性问题，JMM 会**通过插入对应类型的内存屏障来解决处理器重排序问题**。

CPU 的 L1 和 L2 跟 JMM 的模型原理差不多，使用写缓冲，可以提高执行效率，但也不可避免的产生了顺序问题；

不同处理器对重排序指令的支持不一样，但是 Java 编译器为了保证内存可见性，会在生成的字节码适当的位置插入内存屏障来禁止重排序：

| 屏障类型             | 示例                        | 说明                                                         |
| -------------------- | --------------------------- | ------------------------------------------------------------ |
| LoadLoad barriers    | Load 1: LoadLoad: Load2     | 确保 Load1数据的装载先于Load2及所有后续装载指令的装载        |
| Store Store barriers | Store1: Store Store: Store2 | 确保 Store1 数据对其他处理器可见(刷新到内存)先于Store2及所有后续存储指令的存储 |
| LoadStore Barriers   | Load1; LoadStore; Store2    | 确保 Load1 数据装载先于 Store2及所有后续的存储指令刷新到内存 |
| StoreLoad barriers   | Store1: StoreLoad: Load2    | 确保 Store1 数据对其他处理器变得可见(指刷新到内存)先于Load2及所有后续装载指令的装载。 <br/>Storeload barriers 会使该屏障之前的所有内存访问指令(存储和装载指令)完成之后,才执行该屏障之后的内存访问指令。 |

可以看出 StoreLoad 是全能的屏障，具有其他三个的效果，同时现代处理器基本都支持，但是开销会比较大；

~~PS：由于不同处理器支持不同，JVM 层面实现内存屏障没有使用 CPU 特定的指令，而是统一使用了 lock 指令。~~

### happens-before

happens-before 是 JMM 对**操作之间的内存可见性的阐述**（既可以是单线程之间也可以是多线程之间），是一个抽象概念，具体的实现开发者不必深究，只需要知道它的规则：

- 程序顺序规则：一个线程中每个操作 happens-before 于该线程中的任意后续操作；
- 监视器锁规则：对于一个锁的解锁 happens-before 于随后对这个锁的加锁；
- volatile 变量规则：对一个     volatile 的写 happens-before 于任意后续对这个 volatile 域的读；
- 传递性：如果 A happens-before B，B happens-before C，那么 A happens-before C；
- start() 规则：如果线程 A 执行操作 ThreadB.start()（启动线程B），那么 A 线程的 ThreadB.start() 操作 happens-before 于线程 B 中的任意操作。
- join() 规则：如果线程 A 执行操作 ThreadB.join() 并成功返回，那么线程 B 中的任意操作 happens-before 于线程 A 从 ThreadB.join() 操作成功返回。

 两个操作具有 happens-before 关系，**并不是一个操作必须在后一个操作之前执行**，而是前一个操作的**结果**对后一个操作可见，也就是说如果结果不变、无数据依赖，对于第一条规则，JMM 允许一定的处理器重排；

> volatile、Synchronized 等语义的实现依赖于 happens-before 规则。

 happens-before 是 JMM 的核心概念，JMM 的原则就是只要不改变程序执行结果（对程序员的承诺），处理器和编译器怎么优化都行（对编译器和处理器的约束）；可以说在程序员看来顺序执行的代码，在编译器和处理器执行过程中，只要不影响结果，可以乱序执行。

具体的约束参考 JSR-133。 

PS：对象的初始化过程是线程安全的，隐含有一个**初始化锁**，即使里面有重排序，对其他线程也是不可见的，单例模式可以采用静态内部类实现（**大部分情况下，延迟初始化并不是好的选择**，它带来了获取的开销）。

### 顺序一致性

存在数据依赖性的操作无论编译器还是处理器都不会对其重排序，例如读后写、写后写、写后读，指的主要是单个处理器情况下。

多线程的情况下，就比较复杂了，例如下面这段程序：

```java
class Concurrent {
  int a = 0;
  boolean flag = false;
  // thread 1 
  private void writer(){
    a = 1; // 1
    flag = true; // 2
  }
  // thread 2
  private void reader(){
    if (flag) { // 3
      int i = a * a; // 4
    }
  }
}
```

最后 i 的结果不好说，因为会重排序，1 和 2，3 和 4 都可能会重排序；

JMM 对正确同步的多线程程序做了内存一致性的保证，这里说的同步是广义上的，包括 Synchronized、volatile、final；

JMM 允许临界区中的代码重排，因为监视器的互斥特性，**即使重排对其他线程也是不可见的**，所以 JMM 会最大限度保证语义正确的情况下允许各种优化；

JMM 保证读操作不会『无中生有』要么是初始值要么是上一次写入的值，这也是为什么堆上分配对象要零值处理；

JMM 不保证对 64 位的 long 和 double 操作的原子性，这跟总线机制有关，处理器发起内存访问时，通过总线事务（包括读事务和写事务）完成，多个处理器同时发起请求会通过总线仲裁方式选出一个来执行，也就是同一时间对内存的访问是串行化的，这保证了内存读写的原子性，这期间如果还有其他处理器发起请求会被直接拒绝；

由于 32 位处理器对 64 位数据进行原子操作有很大开销，所以可以通过分两次 32 位操作完成，但是这样就没有原子性了，JVM 鼓励但不强求保证 64 位的原子性；不过主流的 JVM 都保证了其原子性（不必再使用 volatile 保证其原子性）。

### volatile 内存语义

JMM 为了保证 volatile 的禁止重排，使用的是内存屏障，并且采用了保守的策略：

- 在每个 volatile 写操作的前面插入一个 Store Store 屏障。
- 在每个 volatile 写操作的后面插入一个 StorelLoad 屏障。
- 在每个 volatile 读操作的后面插入一个 Loadload 屏障。
- 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。

它可以保证在任何处理器平台都可以正确实现 volatile 的语义，一般来说编译器会根据具体的上下文省略优化内存屏障，根据不同的处理器也可以执行相应的去除优化（不同的 CPU 对重排的支持不同）；

另外，对于上面演示的并发重排问题，可以给 flag 设置为 volatile，根据 volatile 的 happens-before 规则，可以解决这个问题，相当于是借助 volatile 实现线程之间的消息传递。 

### 锁内存语义与JUC原理

锁的获取与释放与 volatile 的读写内存语义是一样的，释放一个锁，就是向接下来要获取这个锁的线程发送一个共享变量修改的消息（JMM 会对其的本地内存进行无效化）；

以 ReentrantLock 为例，分为公平锁和非公平锁，它其实依赖于 AbstractQueuedSynchronized（AQS），而 AQS 使用了一个整型的 volatile 来维护同步状态（使用 CAS 来更新）；

**volatile 和 CAS 是 JUC 实现的基石**；JUC 下的工具大多都有一个模式：

- 声明共享变量为 volatile
- 使用 CAS 的原子条件更新实现线程之间的同步
- 配合 volatile 的读写和 CAS 所具有的 volatile 读写内存语义实现内存之间的通讯

JUC 下的原子变量类就是以这种方式实现的，高层再以这些类为基础来实现高级功能；

![](../../../img/java/JUC原理.png)

### final 域的内存语义

final 修饰的变量如果在构造方法中赋值，JMM 会保证对 final 的写重排序到构造方法之外（通过插入内存屏障），也就是说，可以保证 final 的变量在其他线程引用的时候已经完成了初始化，而普通变量则不会有这个保障；

同时 JMM 也规定了在读一个对象的 final 域之前一定会先读这个域的对象的引用，如果不为 null，就说明肯定被初始化过了。

可简单理解为，在构造函数返回之前，构造的对象的引用不能被其他线程可见，在 x86 或者 AMD64 的处理器中，因为不支持相关的重排序规则，所以可以省略其内存屏障。

## 并发基础

停止或者说线程交互的一个方式是使用中断（interrupt），不过需要注意的是当抛出中断异常之前（例如 sleep）会将标志位进行重置；终止线程的另一个方式（或者混合使用）是通过设置一个 flag，不过**记得加 volatile 保证可见性**；

至于线程之间的通信，主要是靠等待、通知机制，volatile 倒也可以完成非常局限的功能；

### 等待/通知的经典范式

可简单理解为：等待方（消费方）、通知方（生产者），还是生产者消费者模型；

等待方原则：

1. 获取对象锁
2. 如果条件不满足就调用 wait 方法，被通知后仍然要检查条件（while）
3. 条件满足执行对应的逻辑

通知方原则：

1. 获取对象锁
2. 改变条件
3. 通知所有等待在对象上的线程

伪代码：

```java
// 等待（消费）方
synchronized(obj){
  while(条件不满足){
    obj.wait();
  }
  // 处理逻辑
}

// 通知（生产）方
synchronized(obj){
  // 改变条件
  obj.notifyAll();
}
```

另外，线程之间的数据传输最常用的是管道（Piped），区别与其他 IO，媒介就是内存了，JDK 提供了四种，分为读写字节、字符流。

线程池技术也基本就是维护一个工作队列，当队列为空就全部阻塞，任务提交后会使用 notify 唤醒一个执行；

### 等待超时模式

这一点我感觉跟异步有点相似，虽然不如异步那样优雅，它避免执行过长的时间，超时会直接返回；区别与 join(wait 0) 就是无限等待），它可以更优雅的使用多线程返回。

```java
public synchronized Object get(long mills) {
  long future = System.currentTimeMillis() + mills;
  long remaining = mills;
  while((result == null) && remaining > 0) {
    wait(remaining);
    remaining = future - System.currentTimeMillis();
  }
  return result;
}
```

### 锁

这里指的是实现了 Lock 接口的类，相比 Synchronized 的字节码层面会更有灵活性：

| 特性               | 描述                                                         |
| ------------------ | ------------------------------------------------------------ |
| 尝试非阻塞的获取锁 | 当前线程尝试获取锁,如果这一时刻锁没有被其他线程获取到,则成功获取并持有锁 |
| 能被中断的获取锁   | 与 synchronized不同,获取到锁的线程能够响应中断,当获取到锁的线程被中断时,中断异常将会被抛出,同时锁会被释放 |
| 超时获取锁         | 在指定的截止时间之前获取锁,如果截止时间到了仍旧无法获取锁,则返回 |

然后是 Lock 接口的一些常用方法：

| 方法名称                          | 描述                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| lock                              | 获取锁,调用该方法当前线程将会获取锁,当锁获得后,从该方法返回  |
| lockInterruptibly                 | 可中断地获取锁,和 lock 方法的不同之处在于该方法会响应中断,即在锁的获取中可以中断当前线程 |
| trylock                           | 尝试非阻塞的获取锁,调用该方法后立刻返回,如果能够获取则返回tue,否则返回 false |
| tryLock(long time, TimeUnit unit) | 超时的获取锁,当前线程在以下3种情况下会返回:<br/>①当前线程在超时时间内获得了锁<br/>②当前线程在超时时间内被中断<br/>③超时时间结束,返回 false |
| unlock                            | 释放锁                                                       |
| newCondition                      | 获取等待通知组件,该组件和当前的锁绑定,当前线程只有获得了锁,才能调用该组件的 wait 方法,而调用后,当前线程将释放锁 |

## AQS

之前也说过，AQS （AbstractQueuedSynchronizer）是**锁或者说其他任何同步组件的基础**，它使用一个 volatile 的 int 变量来表示同步状态，通过**内置 FIFO 队列来完成资源获取线程的排队工作**，JUC 是大神 Doug Lea 设计的。

AQS 的**主要使用方式是继承**，然后实现抽象方法来管理同步状态，它也仅仅是提供了一些修改状态的方法，并且保证它们是安全的，有很大的灵活性。

AQS 采用模版方法实现，使用者继承并重写相关方法，然后组合在自己的同步组件中进行使用。

可重写的方法：

| 方法名            | 描述                                                         |
| ----------------- | ------------------------------------------------------------ |
| tryAcquire        | 独占式获取同步状态,实现该方法需要査询当前状态并判断同步状态是否符合预期,然后再进行CAS设置同步状态 |
| tryRelease        | 独占式释放同步状态,等待获取同步状态的线程将有机会获取同步状态 |
| tryAcquireShared  | 共享式获取同步状态，返回值大于等于0则表示获取成功，否则获取失败； |
| tryReleaseShared  | 共享式释放同步状态；                                         |
| isHeldExclusively | 当前同步器是否在独占式模式下被线程占用，一般该方法表示是否被当前线程所独占； |

提供的模版方法（可供重写的方法调用）：

| 方法名                     | 描述                                                         |
| -------------------------- | ------------------------------------------------------------ |
| acquire                    | 独占式获取同步状态，如果当前线程获取同步状态成功，则由该方法返回，否则，将会进入同步队列等待，该方法将会调用可重写的tryAcquire(int arg)方法 |
| acquireInterruptibly       | 与acquire(int arg)相同，但是该方法响应中断，当前线程为获取到同步状态而进入到同步队列中，如果当前线程被中断，则该方法会抛出InterruptedException异常并返回； |
| tryAcquireNanos            | 超时获取同步状态，如果当前线程在nanos时间内没有获取到同步状态，那么将会返回false，已经获取则返回true |
| acquireShared              | 共享式获取同步状态，如果当前线程未获取到同步状态，将会进入同步队列等待，与独占式的主要区别是在同一时刻可以有多个线程获取到同步状态 |
| acquireSharedInterruptibly | 共享式获取同步状态，响应中断                                 |
| tryAcquireSharedNanos      | 共享式获取同步状态，增加超时限制；                           |
| release                    | 独占式释放同步状态，该方法会在释放同步状态之后，将同步队列中第一个节点包含的线程唤醒； |
| releaseShared              | 共享式释放同步状态                                           |
| getQueuedThreads           | 获取等待在同步队列上的线程集合                               |

模板方法基本分为三类：独占式获取与释放、共享式获取与释放、查询同步队列等待线程情况；

所谓独占式就是同一时刻只能有一个线程获取到锁，其他线程只能处于等待队列中等待；

![](../../../img/java/AQS.png)

### 等待队列

上面提到过，AQS 内部维护了一个 FIFO 的双向队列，当线程 lock 失败时，会进入这个队列，同时会结合其他的信息封装成一个 Node，结构大概是：

| 方法/属性名     | 描述                                                         |
| --------------- | ------------------------------------------------------------ |
| waitstatus      | 等待状态，包含如下状态。<br/>1.CANCELLED,值为1,由于在同步队列中等待的线程等待超时或者被中断,需要从同步队列中取消等待,节点进入该状态将不会变化<br/>2.SIGNAL,值为-1,后继节点的线程处于等待状态,而当前节点的线程如果释放了同步状态或者被取消,将会通知后继节点,使后继节点的线程得以运行<br/>3.CONDITION,值为-2,节点在等待队列中,节点线程等待在 Condition上,当其他线程对Condition调用了 signalo方法后,该节点将会从等待队列中转移到同步队列中,加入到对同步状态的获取中<br/>4.PROPAGATE,值为-3,表示下一次共享式同步状态获取将会无条件地被传播下去<br/>5.INITIAL,值为0,初始状态 |
| Node prev       | 前驱节点,当节点加入同步队列时被设置(从尾部添加)              |
| Node next       | 后继节点                                                     |
| Node nextWaiter | 等待队列中的后继节点。如果当前节点是共享的,那么这个字段将是一个 SHARED常量,也就是说节点类型(独占和共享)和等待队列中的后继节点共用同一个字段 |
| Thread thread   | 获取同步状态的线程                                           |

新加入的 Node 会成为尾节点，为了防止并发问题，设置尾节点的过程提供了一个 CAS 操作，而头始终指向获得锁的线程，当线程执行完毕，会唤醒下一个节点，然后下一个节点会将自己设置为新的首节点，这个过程就不需要 CAS 了（因为相当于持有锁）。

![](../../../img/java/aqs-1.png)

### 独占式状态

通过 AQS 的 acquire 方法获取同步状态，该方法对中断不敏感（中断不会使其移出同步队列）；

它会回调自定义的 tryAcquire 方法，安全的获取同步状态，如果获取失败会封装成 Node 加入同步队列的尾部（CAS 操作完成），接着会调用 acquireQueued 方法不断的获取同步状态（自旋，根据前驱节点满足一定条件会阻塞避免浪费资源），等到前驱节点是头结点才能够进行尝试获取同步状态（锁）。

![](../../../img/java/aqs-2.png)

这样同步队列中的线程不需要通信，只需要检查前驱节点是否为头结点。

最开始看到如果获取不到同步状态就会自旋，那么为了避免 CPU 的过度消耗，肯定不能一直这样下去，关于是否阻塞或者说什么条件会阻塞：

![](../../../img/java/aqs-3.png)

要看懂，需要配合前面说的 waitstatus 属性的意义。

关于公平与非公平，实现上，公平锁仅仅是在判断的时候多一个 hasQueuedPredecessors 的调用，它的作用就是判断是否还有前驱节点，如果有，表示有线程更早的请求锁（本线程是插队操作），会直接等待进入排队；

这里可能有点绕，简单理解为如果同步队列为：h -> A -> B -> C，这时候新的 D 或者 E 等也来获取锁，那么就会与 A 竞争，如果后来的 D 获取到了，就成了非公平锁（h [-> A] -> D -> B -> C）；同步队列的头有获取同步状态的权利，不是一定会获得锁。

---

对于具有超时功能的独占，对应 AQS 的 doAcquireNanos 方法，为了防止提前唤醒，进行了时间方面的判断，当时间小于 1000 纳秒时就不会进行等待而是进入快速自旋（这样更加精确）；

所以它们的区别就是未获取到同步状态时的处理逻辑。

![](../../../img/java/aqs-4.png)

PS：另外 AQS 还提供了 acquireInterruptibly 方法，在等待获取同步状态的时候也能感知中断，被中断会立即返回，并抛出 InterruptedException。

### 共享式状态

这个参考读写锁原理，基本一致，同一时刻可以多个线程同时获得同步状态，共享式访问资源时其他共享式的访问被允许，但是独占式被阻塞，反之独占式访问资源，不论独占还是共享都阻塞。

对应 AQS 的 acquireShared 方法，主要逻辑上跟独占式差不多，就是多了个 state 表示数量的语义。

### 读写锁

看名字得包括两把锁，也就意味着在 AQS 的 int 状态变量中维护多个读线程和一个写线程的状态；支持重入和锁降级（写-读-释放写就会从写锁降级为读锁）。

因为 AQS 内部只有一个 state 表示状态，这样就只能使用『按位切割使用』来表示两把锁，高 16 位表示读（S >>> 16），低 16 位表示写（S & 0x0000FFFF）。

更新操作就是：写状态更新：S + 1；读状态更新： S + (1 >>> 16) or S + 0x00010000。

### LockSupport工具类
当需要阻塞或者唤醒一个线程时候需要使用 LockSupport 工具的静态方法；

| 方法名    | 描述                                                         |
| --------- | ------------------------------------------------------------ |
| park      | 阻塞当前线程,如果调用 unpark 方法或者当前线程被中断,才能从 park 方法返回 |
| parkNanos | 阻塞当前线程,最长不超过 nanos 纳秒,返回条件在 park 的基础上增加了超时返回 |
| parkUntil | 阻塞当前线程,直到 deadline 时间(从1970年开始到 deadline时间的毫秒数) |
| unpack    | 唤醒处于阻塞状态的线程                                       |

JDK6 后提供了 park(obj/this) 等方法支持传入阻塞对象，以此来替代之前的无参 park，这样在线程 dump 的时候不会发生信息缺失。

### Condition接口
如果需要类似 Synchronized + Object#wait 类似的等待通知模式，可以使用 Condition 对象完成，它的使用和 Object 的监视器方法基本相同，Condition 只能通过 lock 对象获得（应当与锁绑定）。

| 对比项                                             | Object监视器方法            | Codition                                                     |
| -------------------------------------------------- | --------------------------- | ------------------------------------------------------------ |
| 前置条件                                           | 获取对象的锁（隐式获取）    | 调用Lock()获取锁，然后调用Lock.newCondition()获取Condition对象 |
| 调用方式                                           | 直接调用，如：object.wait() | 直接调用，如：condition.await()                              |
| 等待队列个数                                       | 一个                        | 多个（这里思考一下为什么会有多个等待队列？）                 |
| 当前线程释放锁进入等待状态                         | 支持                        | 支持                                                         |
| 当前线程释放锁进入等待状态，在等待状态中不响应中断 | 不支持                      | 支持                                                         |
| 当前线程释放锁并进入超时等待状态                   | 支持                        | 支持                                                         |
| 当前线程释放锁并进入等待状态到将来的某个时间       | 不支持                      | 支持                                                         |
| 唤醒等待队列中的某个线程                           | 支持                        | 支持                                                         |
| 唤醒等待队列中的全部线程                           | 支持                        | 支持                                                         |

常用方法一览：

| 方法名               | 描述                                                         |
| -------------------- | ------------------------------------------------------------ |
| await                | 当前线程进入等待状态直到被通知(signal)或中断,当前线程将进入运行状态且从 await 方法返回的情况,包括:<br/>其他线程调用该 Condition的 signal 或 signalAll 方法,而当前线程被选中唤醒<br/>1.其他线程(调用 interrupt方法)中断当前线程<br/>2.如果当前等待线程从 await 方法返回,那么表明该线程已经获取了 Condition 对象所对应的锁 |
| awaitUninterruptibly | 当前线程进入等待状态直到被通知,从方法名称上可以看出该方法对中断不敏感 |
| awaitNanos           | 当前线程进入等待状态直到被通知、中断或者超时。返回值表示剩余的时间,如果在 nanos Timeout纳秒之前被唤醒,那么返回值就是 (nanosTimeout-实际耗时)。如果返回值是0或者负数,那么可以认定已经超时了 |
| awaitUntil           | 当前线程进入等待状态直到被通知、中断或者到某个时间。如果没有到指定时间就被通知,方法返回tue,否则,表示到了指定时间,方法返回 false |
| signal               | 唤醒一个等待在 Condition 上的线程,该线程从等待方法返回前必须获得与 Condition 相关联的锁 |
| signalAll            | 唤醒所有等待在 Condition 上的线程,能够从等待方法返回的线程必须获得与 Condition 相关联的锁 |

它内部维护了一个 FIFO 的等待队列，复用了 AQS 的 Node 封装，等待队列的更新不需要使用 CAS，因为前提已经获得了锁。

![](../../../img/java/aqs-5.png)

可简单理解为：当调用 await 时，从同步队列的首节点（获得了锁的节点）移动到了等待队列中，等待队列中的线程可感知中断；

使用 signal 唤醒线程默认会唤醒最长等待时间的线程，也就是首节点，其中会先进行 isHeldExclusively 检查是否获取了锁，然后会将其移动到同步队列（队尾）然后使用 LockSupport 唤醒该节点线程。

调用 signalAll 其实就是将等待队列中的节点挨个调用一次 signal，然后将其唤醒。

## JUC
众所周知 HashMap 是不安全的，在并发情况下会导致 put 死循环，导致 CPU 100% 的尴尬局面，那是因为多线程会导致内部的 Entry 链表形成环形结构，这样 next 永远不会为空。

ConcurrentHashMap 在 1.7 版本之前使用分段锁，并且为了减少只用到低位 hash 的问题，进行了再散列；

在 get 中，所有的共享变量全部使用了 volatile，所以不需要加锁，happens-before 原则会保证写优先于读；其实当写不依赖于原值时，也可以多线程写。

扩容为了更加高效，会以 Segment 为单位进行扩容，也是两倍；获取 size 的时候因为是分 Segment，即使使用 volatile 将各个分段相加也是不安全的，但是这种情况比较少，所以它采用了两次不加锁之间相加，如果相同就顺利返回，否则就执行加锁相加（锁住所有的修改方法）；另外它通过 modCount 来感知在统计过程中容器大小发生变化的情况。

---

在 1.8 中，跟随 HashMap 使用了红黑树，并且放弃了 Segment 而使用 CAS + synchronized。

不同于 JDK7 中 segment 的概念，JDK8 中直接用链表的头节点做为锁（synchronized (f)）。

而对于扩容，主要的思路为：首先 new 一个新的 hash 表 (nextTable) 出来，大小是原来的 2 倍。后面的 rehash 都是针对这个新的 hash 表操作，不涉及原 hash 表 (table)。

然后会对原 hash 表 (table) 中的每个链表进行 rehash，此时会尝试获取头节点的锁。这一步就保证了在 rehash 的过程中不能对这个链表执行 put 操作。

所涉及的共享变量更新通过 CAS 完成。

其他比较有意思的，例如在 ConcurrentLinkedQueue （无界，非阻塞，CAS）使用巧妙的 HOPS 来提高更新尾节点（或者出队列）的效率（简单说就是并不是每次都更新尾节点，等到距离 HOPS 的时候才更新，否则就是顺着 next 去找，避免了频繁的更新操作）。

另一个比较有意思的 DelayQueue 是支持延时获取元素的无界阻塞队列，使用 PriorityQueue 实现，队列中的元素必须实现 Delayed 接口，可以用来做缓存和定时任务；当能获取到有效期或者执行时间的时候，意味着可以清除缓存或者开始执行任务。

其他参考之前写过的博客。

## 线程池

线程池其实就是一种（更加聪明，只使用了一个阻塞队列）生产者消费者模型。

线程池的处理流程：

- 判断核心线程池里的线程是否都在执行任务，如果不是就创建新的工作线程来执行任务（需要获取全局锁）；
- 判断工作队列是否已经满了，如果没满就保存到工作队列中；
- 判断线程池的线程是否都处于工作状态，如果没有创建一个新的工作线程来执行任务（需要获取全局锁），如果满了交给饱和策略来处理这个任务；

对于 CPU 密集型，没有阻塞，一直全速运行，一般设置为核数+1；

IO密集型，一般并不是一直在执行任务（有阻塞），一般为核数*2或者核数/(1-阻塞系数)，阻塞系数在0.8-0.9；

线程池的核心为 ThreadPoolExecutor 对象，其他的常见问题也都是面试常客，不再多说。

在任务或者周期线程池的实现中，使用了之前说的 DelayQueue 作为任务队列，它进行了一定的封装，具有排序功能，将 time 最小的放在最前面。

PS：使用 guava 的 ThreadFactoryBuilder 可以快速给线程池里的线程设置名字。

---

对于 FutureTask：

多个线程执行同一个 FutureTask，最终只会执行一次;

同时，它可以感知中断，例如 cancel 方法，传入 true 会以中断的方式试图终止任务。

## 参考

[从ReentrantLock的实现看AQS的原理及应用](https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html)

[性能查看命令](https://github.com/bfchengnuo/MyRecord/issues/29#issuecomment-640240703)