# etcd介绍

> 基于原文 https://www.oneops.co/2019/05/20/getting-started-etcd.html 修改

`etcd`是`CoreOS`团队在 2013 年 6 月发起的一个管理配置信息和服务发现服务 `service discovery` 的开源项目。

etcd 是一个开源的、分布式的键值对数据存储系统，提供共享配置、服务的注册和发现。

它的目标是构建一个高可用的分布式键值 `key-value` 数据库，基于 `GO` 语言实现。在分布式系统中，各种服务的配置信息的管理分享、服务的发现是一个很基本同时也很重要的问题。`CoreOS` 项目就是希望基于 `etcd` 来解决这个问题。

受到[Apache ZooKeeper](http://zookeeper.apache.org/)项目和[doozer](https://github.com/ha/doozerd)项目的启发，`etcd`在设计的时候重点考虑 4 个要素

- **简单**: 具有定义良好，面向用户的 API [gRPC](https://github.com/grpc/grpc)
- **安全**: 支持 `HTTPS` 方式的访问
- **快速**: 支持并发 `10 K/s` 的写操作
- **可靠**: 支持分布式结构，基于 `raft` 的一致性算法

`Apache Zookeeper` 是一套知名的分布式系统中进行通过和已执行管理的工具。

`doozer` 是一个一致性分布式数据库

[`Raft`](https://raft.github.io/) 是一套通过选举主节点来实现分布式系统一致性的算法，相比大名鼎鼎的 `Paxos` 算法，它的过程更容易让人理解。一般情况下，用户在使用 `etcd` 可以在多个节点上启动多个实例，并添加他们为一个集群。同一个集群中的 `etcd` 实例将会保持彼此信息的一致性。

## 使用场景

分布式系统中的数据分为控制数据和应用数据。`etcd` 的使用场景默认处理的数据都是控制数据，对于应用数据，只推荐数量很小，但是更新访问频繁的情况

- 服务注册发现
- 配置管理
- 消息发布和订阅
- 负载均衡
- 分布式通知与协调
- 分布式锁、分布式队列
- 集群监控和 leader 竞选

etcd较多的应用常见便是用于服务发现，服务发现要解决的是分布式系统中最为常见的问题之一，即在同一个分布式集群中的进程或服务如何才能找到对方并建立连接。从本质来说，服务发现就是要了解集群中是否有进程监听了 UDP 或 TCP 端口，并通过名字就可以进行查找和连接。

要解决服务发现问题，需要三大支柱，缺一不可。

- 一个强一致性、高可用的服务存储目录

基于 RAFT 算法的 ETCD 天生就是这样一个强一致性、高可用的服务存储目录。

- 一种注册服务和检查服务监控状态的机制

用户可以通过在 etcd 中注册服务，并对注册服务配置 key TTL，定时保持服务的心跳已达到监控健康状态的效果。

- 一种查找和连接服务的机制

通过在 etcd 指定的主题下注册的服务能在对应的主题下查找。为了确保连接，可以在每个服务器上部署一个 proxy 模式的 etcd，这样就可以确保访问 etcd 集群的服务能够相互连级。

![etcd-service-discovery](https://www.oneops.co/images/etcd/etcd-service-discovery.jpg)

例如你需要一个分布式存储仓库来存储配置信息，并要求这个仓库读写快、支持高可用、部署简单、支持 HTTP 接口，那么就可以使用 `etcd`。目前， cloudfoundry 使用 etcd 作为 hm9000 的应用状态信息存储；kubernetes 使用 etcd 来存储 docker 集群的配置信息。
可参考[etcd：从应用场景到实现原理的全方位解读](https://www.infoq.cn/article/etcd-interpretation-application-scenario-implement-principle)

## ETCD vs ZooKeeper

- 一致性协议： ETCD使用 [Raft] 协议， ZooKeeper 使用 ZAB（类PAXOS协议），前者容易理解，方便工程实现；
- 运维方面：ETCD 方便运维，ZooKeeper 难以运维；
- 项目活跃度：ETCD 社区与开发活跃，ZooKeeper 已经快死了；
- API：ETCD 提供 HTTP+JSON, gRPC 接口，跨平台跨语言，ZooKeeper 需要使用其客户端；
- 访问安全方面：ETCD 支持 HTTPS 访问，ZK 在这方面缺失；

## ETCD vs Redis

1. redis 没有版本的概念，历史版本数据在大规模微服务中非常有必要，对于状态回滚和故障排查，甚至定锅都很重要
2. redis 的注册和发现目前只能通过 pub 和 sub 来实现，这两个命令完全不能满足生产环境的要求
3. etcd 在 2.+版本时，watch 到数据官方文档均建议再 get 一次，因为会存在数据延迟，3.+版本不再需要，可想 redis 的 pub 和 sub 能否达到此种低延迟的要求
4. 一大部分的微服务架构应该都是将 etcd 直接暴露给 client 和 server 的，etcd 的性能摆在那，能够承受多少的 c/s 直连呢，更好的做法应该是对 etcd 做一层保护，当然这种做法会损失一些功能
5. redis 和 etcd 的集群实现方案是不一致的，etcd 采用的是 raft 协议，一主多从，只能写主，底层采用 boltdb 作为 k/v 存储，直接落盘
6. redis 的持久化方案有 aof 和 rdb，这两种方案在宕机的时候都或多或少的会丢失数据

总结，Redis 从来没有想过抢 etcd 在服务注册和发现的饭碗，目前的架构来说也抢不动，在缓存方面目前在性能和功能也无出其右； etcd 只关注在服务注册与发现方面，非要当做 k/v 存储来用（丢弃 watch 特性而言）也可以用，性能也不错，但只能说你选错对象了

微服务架构中选择哪种技术，取决于技术决定人的规划理想，非要说 redis 不行当然也是错的，不考虑服务规模和性能需求，用 mongodb 也能搞，mongodb 3.6+版本也有个 oplog watch 的功能

## ETCD 工作原理

![etcd_principle](https://www.oneops.co/images/etcd/etcd_principle.png)
![etcd_role](https://www.oneops.co/images/etcd/etcd_role.png)

ETCD 使用 Raft 协议来维护集群内各个节点状态的一致性。简单说，ETCD 集群是一个分布式系统，由多个节点相互通信构成整体对外服务，每个节点都存储了完整的数据，并且通过 Raft 协议保证每个节点维护的数据是一致的。
如图所示，每个ETCD节点都维护了一个状态机，并且，任意时刻至多存在一个有效的主节点。主节点处理所有来自客户端写操作，通过Raft协议保证写操作对状态机的改动会可靠的同步到其他节点。

Raft协议主要分为三个部分：选主，日志复制，安全性。

### 选主

Raft 协议是用于维护一组服务节点数据一致性的协议。这一组服务节点构成一个集群，并且有一个主节点来对外提供服务。当集群初始化，或者主节点挂掉后，面临一个选主问题。集群中每个节点，任意时刻处于 Leader, Follower, Candidate 这三个角色之一。选举特点如下：

- 当集群初始化时候，每个节点都是Follower角色；
- 集群中存在至多1个有效的主节点，通过心跳与其他节点同步数据；
- 当Follower在一定时间内没有收到来自主节点的心跳，会将自己角色改变为 Candidate，并发起一次选主投票；当收到包括自己在内超过半数节点赞成后，选举成功；当收到票数不足半数选举失败，或者选举超时。若本轮未选出主节点，将进行下一轮选举（出现这种情况，是由于多个节点同时选举，所有节点均为获得过半选票）。
- Candidate节点收到来自主节点的信息后，会立即终止选举过程，进入Follower角色。
- 为了避免陷入选主失败循环，每个节点未收到心跳发起选举的时间是一定范围内的随机值，这样能够避免2个节点同时发起选主。

### 日志复制

所谓日志复制，是指主节点将每次操作形成日志条目，并持久化到本地磁盘，然后通过网络 `I\O` 发送给其他节点。其他节点根据日志的逻辑时钟 TERM 和日志编号 INDEX 来判断是否将该日志记录持久化到本地。当主节点收到包括自己在内超过半数节点成功返回，那么认为该日志是可提交的 committed，并将日志输入到状态机，将结果返回给客户端。

这里需要注意的是，每次选主都会形成一个唯一的 TERM 编号，相当于逻辑时钟。每一条日志都有全局唯一的编号。

![etcd_replication](https://www.oneops.co/images/etcd/etcd_replication.png)

### 安全性

选主以及日志复制并不能保证节点间数据一致。试想，当一个某个节点挂掉了，一段时间后再次重启，并当选为主节点。而在其挂掉这段时间内，集群若有超过半数节点存活，集群会正常工作，那么会有日志提交。这些提交的日志无法传递给挂掉的节点。当挂掉的节点再次当选主节点，它将缺失部分已提交的日志。在这样场景下，按 Raft 协议，它将自己日志复制给其他节点，会将集群已经提交的日志给覆盖掉。

这显然是不可接受的。

其他协议解决这个问题的办法是，新当选的主节点会询问其他节点，和自己数据对比，确定出集群已提交数据，然后将缺失的数据同步过来。这个方案有明显缺陷，增加了集群恢复服务的时间（集群在选举阶段不可服务），并且增加了协议的复杂度。

Raft 解决的办法是，在选主逻辑中，对能够成为主的节点加以限制，确保选出的节点已定包含了集群已经提交的所有日志。如果新选出的主节点已经包含了集群所有提交的日志，那就不需要从和其他节点比对数据了。简化了流程，缩短了集群恢复服务的时间。

这里存在一个问题，加以这样限制之后，还能否选出主呢？答案是：只要仍然有超过半数节点存活，这样的主一定能够选出。因为已经提交的日志必然被集群中超过半数节点持久化，显然前一个主节点提交的最后一条日志也被集群中大部分节点持久化。当主节点挂掉后，集群中仍有大部分节点存活，那这存活的节点中一定存在一个节点包含了已经提交的日志了。

## ETCD 使用案例

据公开资料显示，至少有 CoreOS, Google Kubernetes, Cloud Foundry, 以及在 Github 上超过 500 个项目在使用 ETCD。

## ETCD 接口

ETCD 提供 HTTP 协议，在最新版本中支持 Google gRPC 方式访问。具体支持接口情况如下：

- ETCD 是一个高可靠的 KV 存储系统，支持 PUT/GET/DELETE 接口；
- 为了支持服务注册与发现，支持 WATCH 接口（通过http long poll实现）；
- 支持 KEY 持有 TTL 属性；
- CAS（compare and swap)操作;
- 支持多 key 的事务操作；
- 支持目录操作

## 对于SC

| Feature              | Consul                 | zookeeper             | etcd              | euerka                       |
| :------------------- | :--------------------- | :-------------------- | :---------------- | :--------------------------- |
| 服务健康检查         | 服务状态，内存，硬盘等 | (弱)长连接，keepalive | 连接心跳          | 可配支持                     |
| 多数据中心           | 支持                   | —                     | —                 | —                            |
| kv存储服务           | 支持                   | 支持                  | 支持              | —                            |
| 一致性               | raft                   | paxos                 | raft              | —                            |
| cap                  | ca                     | cp                    | cp                | ap                           |
| 使用接口(多语言能力) | 支持http和dns          | 客户端                | http/grpc         | http（sidecar）              |
| watch支持            | 全量/支持long polling  | 支持                  | 支持 long polling | 支持 long polling/大部分增量 |
| 自身监控             | metrics                | —                     | metrics           | metrics                      |
| 安全                 | acl /https             | acl                   | https支持（弱）   | —                            |
| spring cloud集成     | 已支持                 | 已支持                | 已支持            | 已支持                       |